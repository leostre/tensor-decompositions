{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "996cae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "TDECOMP_PATH = '..'\n",
    "if not TDECOMP_PATH in sys.path:\n",
    "    sys.path.append(TDECOMP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618202bd",
   "metadata": {},
   "source": [
    "Берём SmallLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db5da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ptls-experiments/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"arnir0/Tiny-LLM\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,  use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20db1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Dataset size: 1000\n",
      "Dataset features: {'text': Value(dtype='string', id=None)}\n",
      "Training samples: 517\n",
      "Validation samples: 130\n"
     ]
    }
   ],
   "source": [
    "# train_imdb.py\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "model_name = 'arnir0/Tiny-LLM'\n",
    "dataset_name = \"imdb\"\n",
    "# output_dir = \"./qwen2-0.5b-imdb-finetuned\"\n",
    "output_dir = './tiny-llm'\n",
    "max_length = 512  # Maximum context length for each sample\n",
    "\n",
    "# Use 4-bit quantization to drastically reduce memory usage\n",
    "use_4bit = False\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "# LoRA configuration for Parameter-Efficient Fine-Tuning\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Training arguments\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 2e-4\n",
    "logging_steps = 10\n",
    "save_steps = 500\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
    "# Set padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 4-bit quantization if enabled\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "if use_4bit:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Automatically places layers on available GPUs\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\", #auto\n",
    "        torch_dtype=torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# # Option 1: Tiny Shakespeare (literary text)\n",
    "# dataset = load_dataset(\"tiny_shakespeare\", split=\"train[:5%]\")  # First 5%\n",
    "\n",
    "# Option 2: CNN Daily Mail (news summaries) - smaller subset\n",
    "# dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:100]\")\n",
    "\n",
    "# Option 3: Wikitext (Wikipedia articles) - small subset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")\n",
    "\n",
    "# Option 4: Twitter Complaints (short text)\n",
    "# dataset = load_dataset(\"twitter_complaints\", split=\"train[:200]\")\n",
    "\n",
    "# Option 5: AG News (news articles)\n",
    "# dataset = load_dataset(\"ag_news\", split=\"train[:100]\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "\n",
    "# Preprocess the dataset based on its structure\n",
    "def preprocess_dataset(examples):\n",
    "    \"\"\"Extract text from different dataset formats\"\"\"\n",
    "    if 'text' in examples:\n",
    "        return {\"text\": examples[\"text\"]}\n",
    "    elif 'article' in examples:  # CNN Daily Mail\n",
    "        return {\"text\": examples[\"article\"]}\n",
    "    elif 'content' in examples:  # Some datasets\n",
    "        return {\"text\": examples[\"content\"]}\n",
    "    elif 'sentence' in examples:  # Some sentence datasets\n",
    "        return {\"text\": examples[\"sentence\"]}\n",
    "    else:\n",
    "        # Try to use the first string column\n",
    "        for key, value in examples.items():\n",
    "            if isinstance(value[0], str):\n",
    "                return {\"text\": examples[key]}\n",
    "        return {\"text\": [str(x) for x in examples[list(examples.keys())[0]]]}\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_dataset, batched=True)\n",
    "\n",
    "# Filter out empty texts\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"].strip()) > 0)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,  # Reduced for tiny model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "\n",
    "\n",
    "# This will dynamically pad the batches during training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We are doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Load accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Shift labels and predictions for causal LM (next token prediction)\n",
    "    # Predictions are for the next token, so we shift labels accordingly\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    \n",
    "    # Flatten the tokens and get predictions\n",
    "    predictions = np.argmax(shift_logits, axis=-1).flatten()\n",
    "    labels = shift_labels.flatten()\n",
    "    \n",
    "    # Calculate accuracy, ignoring padding tokens (where label = -100)\n",
    "    mask = labels != -100\n",
    "    predictions = predictions[mask]\n",
    "    labels = labels[mask]\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Training Arguments\n",
    "# ----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    # logging_steps=logging_steps,\n",
    "    logging_steps=5,\n",
    "    save_steps=save_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,  # Disable external logging like Weights & Biases for simplicity\n",
    "    fp16=False,  # Use mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa48bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdecomp.grad_proj.tensorgrad.config import TensorGRaDConfig, DataConfig, OptimizerConfig\n",
    "import tdecomp.matrix.functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb343094",
   "metadata": {},
   "source": [
    "ParallelTG, ULTG - это вспомогательные классы-фабрики, убирающие лишние настройки, чтобы было проще. \n",
    "Если понадобится более тонкая настройка -- смотрите TensorGRaDConfig и передавайте соответствующие поля (там есть **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65897751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdecomp.grad_proj.tensorgrad.prepared_tg import ParallelTG, ULTG\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=ParallelTG(model, \n",
    "                    F.cur, # svd_type можете зарегистрировать свои разложения матричные в модуле F, \n",
    "                    # а также просто `truncated_svd` и `randomized_svd`\n",
    "                    (8, 0.5), # первый и второй ранги. может быть Number, тогда ранги ставятся одинкаовыми.\n",
    "                    # 0 < float < 1 интерпретируется как доля параметров, int - непосредственно ранг\n",
    "                                  n_train=len(train_dataset),\n",
    "                                  batch_size=per_device_train_batch_size,\n",
    "                                  scheduler='StepLR'\n",
    "                                  ),\n",
    "    # compute_metrics=compute_metrics, # Uncomment for per-epoch metrics (slower)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15104a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91f5ad40>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91f5b580>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fec3d0>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fec550>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fec3a0>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fec6a0>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fecb50>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fecc70>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n",
      "### Using Composite Projector Configuration ###\n",
      "    => Swapping projectors to ensure smaller one is first\n",
      "    => Sizes after swap: first=0.25, second=8.0\n",
      "Update gap scheduler: <tdecomp.grad_proj.tensorgrad.projectors.projector_utils.UpdateGapScheduler object at 0x7efb91fecd90>\n",
      "UnstructuredSparseProjector initialized with sparse_ratio=0.25, sparse_type=topk, scale_by_mask_ratio=False\n",
      "    => First projector: unstructured_sparse\n",
      "    => Second projector: low_rank\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.330400</td>\n",
       "      <td>5.432490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.235200</td>\n",
       "      <td>5.362298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.208000</td>\n",
       "      <td>5.290714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.143700</td>\n",
       "      <td>5.216618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.065500</td>\n",
       "      <td>5.143417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.927600</td>\n",
       "      <td>5.076861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.005400</td>\n",
       "      <td>5.070557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.895100</td>\n",
       "      <td>5.064242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.927600</td>\n",
       "      <td>5.057898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.032900</td>\n",
       "      <td>5.051560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>4.964900</td>\n",
       "      <td>5.045249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.969800</td>\n",
       "      <td>5.038963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>4.877900</td>\n",
       "      <td>5.038339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.940900</td>\n",
       "      <td>5.037720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.930900</td>\n",
       "      <td>5.037105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.892000</td>\n",
       "      <td>5.036488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>4.902800</td>\n",
       "      <td>5.035873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.981700</td>\n",
       "      <td>5.035257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>4.879600</td>\n",
       "      <td>5.035196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=99, training_loss=4.999285476376312, metrics={'train_runtime': 6.9874, 'train_samples_per_second': 221.971, 'train_steps_per_second': 14.168, 'total_flos': 8153535430656.0, 'train_loss': 4.999285476376312, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5074fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPE pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
